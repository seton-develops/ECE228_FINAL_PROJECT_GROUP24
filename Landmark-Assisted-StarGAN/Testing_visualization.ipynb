{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2d6d3c6",
   "metadata": {},
   "source": [
    "## You can test and visualize our model using this notebook. You should run each cell in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831adae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from utils import save_checkpoint, load_checkpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import config\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from model_D import Discriminator\n",
    "from model_G import Generator\n",
    "from model_R import Regressor\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23af61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_H = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "disc_C = Discriminator(in_channels=3).to(config.DEVICE)\n",
    "gen_C = Generator(img_channels=3, num_residuals=9).to(config.DEVICE)\n",
    "gen_H = Generator(img_channels=3, num_residuals=9).to(config.DEVICE)\n",
    "reg_C = Regressor().to(config.DEVICE)\n",
    "reg_C.load_state_dict(torch.load('cartoon_torch.pt'))\n",
    "reg_H = Regressor().to(config.DEVICE)\n",
    "reg_H.load_state_dict(torch.load('human_torch.pt')) \n",
    "opt_disc = optim.Adam(\n",
    "        list(disc_H.parameters()) + list(disc_C.parameters()),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "opt_gen = optim.Adam(\n",
    "        list(gen_C.parameters()) + list(gen_H.parameters()),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "load_checkpoint(\n",
    "            config.CHECKPOINT_G_H, gen_H, opt_gen, config.LEARNING_RATE,\n",
    "        )\n",
    "load_checkpoint(\n",
    "            config.CHECKPOINT_G_C, gen_C, opt_gen, config.LEARNING_RATE,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "    def __init__(self, root_human, root_blond, transform=None):\n",
    "        self.root_blond = root_blond\n",
    "        self.root_human = root_human\n",
    "        self.transform = transform\n",
    "    \n",
    "        self.blond_images = os.listdir(root_blond)\n",
    "        self.human_images = os.listdir(root_human)\n",
    "        \n",
    "        self.length_dataset = max(len(self.blond_images), len(self.human_images)) # 1000, 1500\n",
    "        self.blond_len = len(self.blond_images)\n",
    "        self.human_len = len(self.human_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        blond_name = self.blond_images[index % self.blond_len]\n",
    "        blond_path = os.path.join(self.root_blond, blond_name)\n",
    "        \n",
    "        human_name = self.human_images[index % self.human_len]\n",
    "        human_path = os.path.join(self.root_human, human_name)\n",
    "        \n",
    "        blond_img = np.array(Image.open(blond_path).convert(\"RGB\"))\n",
    "        human_img = np.array(Image.open(human_path).convert(\"RGB\"))\n",
    "        \n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=blond_img, image0=human_img)\n",
    "            blond_img = augmentations[\"image\"]\n",
    "            human_img = augmentations[\"image0\"]\n",
    "\n",
    "        return human_img, blond_img, human_name, blond_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_human contains the input images selected from CelebA dataset\n",
    "# root_blond contains the images generated by StarGAN with the images from root_human as inputs. \n",
    "val_dataset = Test(\n",
    "        root_human=\"/home/zluan/ECE_228_project/CycleGAN/stargan_input\", \n",
    "        root_blond=\"/home/zluan/ECE_228_project/CycleGAN/stargan_output\", \n",
    "        transform=config.transforms\n",
    "    )\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original input images and the blond human images\n",
    "loop = tqdm(loader, leave=True)\n",
    "star_im_out = []\n",
    "star_im_out_name = []\n",
    "star_im_in=[]\n",
    "star_im_in_name = []\n",
    "\n",
    "for idx, (human, blond,human_name,blond_name) in enumerate(loop):\n",
    "        star_out = blond.to(config.DEVICE)\n",
    "        star_out_name = blond_name\n",
    "        star_in = human.to(config.DEVICE)\n",
    "        star_in_name = human_name\n",
    "\n",
    "        \n",
    "        star_im_out.append(star_out)\n",
    "        star_im_out_name.append(star_out_name)\n",
    "        star_im_in.append(star_in)\n",
    "        star_im_in_name.append(star_in_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c563805",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6b601",
   "metadata": {},
   "source": [
    "##### You can visualize the result here. You are expected to see some decent results and some more distortion results like what we mentioned in our report that our results are affected by many factors (like complicated background, eyeglasses, darker skin tone and etc.).\n",
    "##### The first row being the output of our model and the input blond human image.\n",
    "##### The second row being the output of our model and the input human image from ClebA dataset. \n",
    "##### Feel free to run the cell below multiple times to check out different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = i+1\n",
    "print(i)\n",
    "print(star_im_out_name[i])\n",
    "star_out_out = star_im_out[i]\n",
    "star_in_out = star_im_in[i]\n",
    "star_image_out = gen_C(torch.squeeze(star_out_out).cuda()*0.5+0.5)\n",
    "star_image_out_initial = torch.squeeze(star_out_out).cuda()*0.5+0.5\n",
    "star_image_in = gen_C(torch.squeeze(star_in_out).cuda()*0.5+0.5)\n",
    "star_image_in_initial = torch.squeeze(star_in_out).cuda()*0.5+0.5\n",
    "\n",
    "star_out_plt = np.transpose(star_image_out.cpu().detach().numpy(), (1, 2, 0))\n",
    "star_out_initial_plt = np.transpose(star_image_out_initial.cpu().detach().numpy(), (1, 2, 0))\n",
    "star_in_plt = np.transpose(star_image_in.cpu().detach().numpy(), (1, 2, 0))\n",
    "star_in_initial_plt = np.transpose(star_image_in_initial.cpu().detach().numpy(), (1, 2, 0))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(star_out_plt*0.5+0.5)\n",
    "plt.axis('off')\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(star_out_initial_plt)\n",
    "plt.axis('off')\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(star_in_plt*0.5+0.5)\n",
    "plt.axis('off')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(star_in_initial_plt)\n",
    "plt.axis('off')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE_228_project",
   "language": "python",
   "name": "ece_228_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
