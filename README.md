# Landmark-Assisted-StarGAN
Project to convert images from CelebA dataset to bitemoji avatars.


## Overview
NOTE: PLEASE DELETE ANY FILE INSIDE `~/trainA`, `~/trainB`, `~/valA`, `~/valB`, and `~/saved_images` folder. Since Github does not allow uploading empty folder, we put an empty txt file there. 

Inside the `Landmark-Assisted-StarGAN` folder:

`~/data` contains the training, testing data, and the landmark coordinate xlsx file we provided.

`~/saved_images` contains the images generated during the training process.

`~/stargan_input` contains the selected input images from CelebA dataset for testing purpose.

`~/stargan_output` contains the selected output images generated by the pre-trained StarGAN model for testing purpose.

`Testing_visualization.ipynb` is for testing the model you trained or the pre-trained model we provided.

`cartoon_torch.pt` is the pre-trained regressor model for cartoon.

`config.py` contains the hyper-parameters, number of epochs, options for load/save models, and etc. You can always change the values to your demand. 

`dataset.py` is the dataloader we created specific for this task.

`human_torch.pt` is the pre-trained regressor model for human.

`local_landmark_discriminator.py` is the local discriminator model.

`model_D.py` is the discriminator model from https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/CycleGAN.

`model_G.py` is the generator model from https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/GANs/CycleGAN.

`model_R.py` is the regressor model.

`patch_extractor.py` contains functions that crop the images into patches.

`train.py` is the second training stage set up for our model (CycleGAN + landmark consistency loss + local discriminator).

`train_first.py` is the first training set up for our model (CycleGAN + landmark consistency loss).

`utils.py` contains the utils functions to save/load models and calculate the landmark consistency loss.

## Download dataset or our pre-trained models

### dataset

We have already provided the xlsx file for the landmark coordinates inside the `~/Landmark-Assisted-StarGAN/data/train(val)` folder. 


You should replace the ~/trainA, ~/trainB, ~/valA, ~/valB folder with the folder you downloaded from our link. 
NOTE: we did not use the val data for validation. The data in `~/valA(B)` folder is for testing.

### pre-trained models
If you want to use the pre-trained models.

Please put evey pre-trained model inside the `~/Landmark-Assisted-StarGAN` folder. You should have total of 10 `.pth.tar` files.



## Training the model


If you want to use a different training setting, feel free to change the hyper-parameters in config.py.

If you want to recreate our training process, run:

`cd ECE228_FINAL_PROJECT_GROUP24/Landmark-Assisted-StarGAN`

Then, run:

`python train_first.py`

The images generated during the training process is saved in saved_images folder. 

After training with 60 epochs, you need to go to config.py and change the NUM_EPOCHS to 40.

Then, run:

`python train.py`

## Test and visualize the model


Go to Testing_visualization.ipynb to test and visualize the result. 
We provided 20 images generated from the pre-trained StarGAN model inside the stargan_input and stargan_out folder.

If you set up everything correctly, you should be able to run the cell in sequence without any problem.










